{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HVtf_Lmu4KD0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dgTgZgWY5iIm"
      },
      "outputs": [],
      "source": [
        "\n",
        "# dataset doesn't have column names, so we have to provide it\n",
        "columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty_level\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BQV0YJ_E5mW8",
        "outputId": "44b421c3-c256-467f-b4df-9ad3b6d28457"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'KDDTrain+.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-30f1436adbe2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# importing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KDDTrain+.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KDDTrain+.txt'"
          ]
        }
      ],
      "source": [
        "\n",
        "# importing dataset\n",
        "data = pd.read_csv('KDDTrain+.txt',header=None, names=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3z3pa_KM5ms5"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f9HkcCSC515E"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "65tzd46r53v8"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "S9yYmjno56RF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# remove attribute 'difficulty_level'\n",
        "data.drop(['difficulty_level'],axis=1,inplace=True)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X-Rt4IAO6AIc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# number of attack labels\n",
        "data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RiR4hIu_6FiO"
      },
      "outputs": [],
      "source": [
        "# changing attack labels to their respective attack class\n",
        "def change_label(df):\n",
        "    df['label'] = df['label'].replace(['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod',\n",
        "                                       'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm'], 'Dos')\n",
        "    df['label'] = df['label'].replace(['ftp_write', 'guess_passwd', 'httptunnel', 'imap',\n",
        "                                       'multihop', 'named', 'phf', 'sendmail', 'snmpgetattack',\n",
        "                                       'snmpguess', 'spy', 'warezclient', 'warezmaster',\n",
        "                                       'xlock', 'xsnoop'], 'R2L')\n",
        "    df['label'] = df['label'].replace(['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'], 'Probe')\n",
        "    df['label'] = df['label'].replace(['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit',\n",
        "                                       'sqlattack', 'xterm'], 'U2R')\n",
        "\n",
        "# calling change_label() function\n",
        "change_label(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pj151c-W6KiG"
      },
      "outputs": [],
      "source": [
        "data.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjK39EI169HC"
      },
      "source": [
        "Data Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AFBJgZVZ6R61"
      },
      "outputs": [],
      "source": [
        "\n",
        "# importing required libraries for normalizing data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CfBFHlaJ7DIH"
      },
      "outputs": [],
      "source": [
        "# selecting numeric attributes columns from data\n",
        "numeric_col = data.select_dtypes(include='number').columns\n",
        "# using standard scaler for normalizing\n",
        "std_scaler = StandardScaler()\n",
        "def normalization(df,col):\n",
        "  for i in col:\n",
        "    arr = df[i]\n",
        "    arr = np.array(arr)\n",
        "    df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "# calling the normalization() function\n",
        "data = normalization(data.copy(),numeric_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UP_6MQ3C7KyS"
      },
      "outputs": [],
      "source": [
        "# data after normalization\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FOzHuYFD7OJY"
      },
      "outputs": [],
      "source": [
        "# selecting categorical data attributes\n",
        "cat_col = ['protocol_type','service','flag']\n",
        "# creating a dataframe with only categorical attributes\n",
        "categorical = data[cat_col]\n",
        "categorical.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C_RJqpNT7V6b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# one-hot-encoding categorical attributes using pandas.get_dummies() function\n",
        "categorical = pd.get_dummies(categorical,columns=cat_col)\n",
        "categorical.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz3dNlAh7aWH"
      },
      "source": [
        "binary classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8iMXcBqX7Wa7"
      },
      "outputs": [],
      "source": [
        "# changing attack labels into two categories 'normal' and 'abnormal'\n",
        "bin_label = pd.DataFrame(data.label.map(lambda x:'normal' if x=='normal' else 'abnormal'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rJbwmig67dkk"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe with binary labels (normal,abnormal)\n",
        "bin_data = data.copy()\n",
        "bin_data['label'] = bin_label\n",
        "le1 = preprocessing.LabelEncoder()\n",
        "enc_label = bin_label.apply(le1.fit_transform)\n",
        "bin_data['intrusion'] = enc_label\n",
        "np.save(\"le1_classes.npy\",le1.classes_,allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gzqWkErl7slE"
      },
      "outputs": [],
      "source": [
        "# dataset with binary labels and label encoded column\n",
        "bin_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pDoFyN987zhj"
      },
      "outputs": [],
      "source": [
        "bin_data = pd.get_dummies(bin_data,columns=['label'],prefix=\"\",prefix_sep=\"\")\n",
        "bin_data['label'] = bin_label\n",
        "bin_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pcm1v-5h73Ln"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pie chart distribution of normal and abnormal labels\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(bin_data.label.value_counts(),labels=bin_data.label.unique(),autopct='%0.2f%%')\n",
        "plt.title(\"Pie chart distribution of normal and abnormal labels\")\n",
        "plt.legend()\n",
        "# plt.savefig('plots/Pie_chart_binary.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNtBT7Oi7-Mr"
      },
      "source": [
        "multiclass classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OZkoCekr7_yw"
      },
      "outputs": [],
      "source": [
        "# Copying data and creating a multi-class label DataFrame\n",
        "multi_data = data.copy()\n",
        "\n",
        "# Creating a DataFrame for the 'label' column and dropping NaN values\n",
        "multi_label = pd.DataFrame(multi_data['label']).dropna()\n",
        "\n",
        "# Dropping corresponding rows from multi_data as well\n",
        "multi_data = multi_data[multi_data['label'].notna()]\n",
        "\n",
        "multi_label.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a3nUtTpB8DEt"
      },
      "outputs": [],
      "source": [
        "le2 = preprocessing.LabelEncoder()\n",
        "enc_label = multi_label.apply(le2.fit_transform)\n",
        "multi_data['intrusion'] = enc_label\n",
        "print(le2.classes_)\n",
        "np.save(\"le2_classes.npy\",le2.classes_,allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tf5ocnb88Hvn"
      },
      "outputs": [],
      "source": [
        "# one-hot-encoding attack label\n",
        "multi_data = pd.get_dummies(multi_data,columns=['label'],prefix=\"\",prefix_sep=\"\")\n",
        "multi_data['label'] = multi_label\n",
        "multi_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4f6Jyogj8JjX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pie chart distribution of multi-class labels\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(multi_data.label.value_counts(),labels=multi_data.label.unique(),autopct='%0.2f%%')\n",
        "plt.title('Pie chart distribution of multi-class labels')\n",
        "plt.legend()\n",
        "# plt.savefig('plots/Pie_chart_multi.png')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "feBAiO_88LgQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# creating a dataframe with only numeric attributes of binary class dataset and encoded label attribute\n",
        "numeric_bin = bin_data[numeric_col]\n",
        "numeric_bin['intrusion'] = bin_data['intrusion']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gNZBA1M39wlv"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr = numeric_bin.corr()\n",
        "\n",
        "# Filter to get only features with correlation greater than 0.5 with 'intrusion'\n",
        "high_corr_features = corr.index[corr['intrusion'].abs() > 0.5]\n",
        "filtered_corr = corr.loc[high_corr_features, high_corr_features]\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(filtered_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title('Heatmap of Features Correlated with Intrusion Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rM_VWO4s90oj"
      },
      "outputs": [],
      "source": [
        "# selecting attributes found by using pearson correlation coefficient\n",
        "numeric_bin = bin_data[['count','srv_serror_rate','serror_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "                         'logged_in','dst_host_same_srv_rate','dst_host_srv_count','same_srv_rate']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b-ja23HBDVKw"
      },
      "outputs": [],
      "source": [
        "# joining the selected attribute with the one-hot-encoded categorical dataframe\n",
        "numeric_bin = numeric_bin.join(categorical)\n",
        "# then joining encoded, one-hot-encoded, and original attack label attribute\n",
        "bin_data = numeric_bin.join(bin_data[['intrusion','abnormal','normal','label']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "412Rbr4jDXhK"
      },
      "outputs": [],
      "source": [
        "bin_data.to_csv(\"bin_data.csv\")\n",
        "# final dataset for binary classification\n",
        "bin_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "f-Noiu0tDjHy"
      },
      "outputs": [],
      "source": [
        "# creating a dataframe with only numeric attributes of multi-class dataset and encoded label attribute\n",
        "numeric_multi = multi_data[numeric_col]\n",
        "numeric_multi['intrusion'] = multi_data['intrusion']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EK2AO7C5Do3q"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr = numeric_multi.corr()\n",
        "\n",
        "# Filter features with an absolute correlation greater than 0.5 with 'intrusion'\n",
        "high_corr_features = corr.index[corr['intrusion'].abs() > 0.5]\n",
        "filtered_corr = corr.loc[high_corr_features, high_corr_features]\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(filtered_corr, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
        "plt.title('Heatmap of Attributes with High Correlation with Intrusion')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iASo0iKkD4RC"
      },
      "outputs": [],
      "source": [
        "# selecting attributes found by using pearson correlation coefficient\n",
        "numeric_multi = multi_data[['count','logged_in','srv_serror_rate','serror_rate','dst_host_serror_rate',\n",
        "                        'dst_host_same_srv_rate','dst_host_srv_serror_rate','dst_host_srv_count','same_srv_rate']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AZGJOyZED-A4"
      },
      "outputs": [],
      "source": [
        "# joining the selected attribute with the one-hot-encoded categorical dataframe\n",
        "numeric_multi = numeric_multi.join(categorical)\n",
        "# then joining encoded, one-hot-encoded, and original attack label attribute\n",
        "multi_data = numeric_multi.join(multi_data[['intrusion','Dos','Probe','R2L','U2R','normal','label']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-j5FnpkVGmfC"
      },
      "outputs": [],
      "source": [
        "multi_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b0gblbIZECb2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# saving final dataset to disk\n",
        "multi_data.to_csv('multi_data.csv')\n",
        "\n",
        "# final dataset for multi-class classification\n",
        "multi_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXyef3v7Ix0t"
      },
      "source": [
        " Binary Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hWpdJYqrJhLa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle # saving and loading trained model\n",
        "from os import path\n",
        "\n",
        "# importing required libraries for normalizing data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# importing library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing library for support vector machine classifier\n",
        "from sklearn.svm import SVC\n",
        "# importing library for K-neares-neighbor classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# importing library for Linear Discriminant Analysis Model\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# importing library for Quadratic Discriminant Analysis Model\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
        "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
        "from sklearn.metrics import classification_report # for generating a classification report of model\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from keras.layers import Dense # importing dense layer\n",
        "from keras.models import Sequential #importing Sequential layer\n",
        "from keras.models import model_from_json # saving and loading trained model\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "# representation of model layers\n",
        "# from keras.utils.vis_utils import plot_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vetMnbNhI1Cw"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "enJ3V18OazfP"
      },
      "outputs": [],
      "source": [
        "# prompt: show me data with Nan values in bin_data and drop that row\n",
        "\n",
        "# Find rows with NaN values in 'bin_data'\n",
        "rows_with_nan = bin_data[bin_data.isnull().any(axis=1)]\n",
        "\n",
        "# Print the rows with NaN values\n",
        "print(\"Rows with NaN values:\")\n",
        "print(rows_with_nan)\n",
        "\n",
        "# Drop rows with NaN values in 'bin_data'\n",
        "bin_data.dropna(inplace=True)\n",
        "\n",
        "# Verify that NaN values are removed\n",
        "print(\"\\nRows with NaN values after dropping:\")\n",
        "print(bin_data[bin_data.isnull().any(axis=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wbdf-HV3EF2u"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "X = bin_data.iloc[:,0:93].to_numpy() # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y = bin_data['intrusion'] # target attribute\n",
        "\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
        "\n",
        "# using kernel as linear\n",
        "lsvm = SVC(kernel='linear',gamma='auto')\n",
        "lsvm.fit(X_train,y_train) # training model on training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QIBp-PwfFuTd"
      },
      "outputs": [],
      "source": [
        "pkl_filename = \"lsvm_binary.pkl\"\n",
        "if (not path.isfile(pkl_filename)):\n",
        "  # saving the trained model to disk\n",
        "  with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(lsvm, file)\n",
        "  print(\"Saved model to disk\")\n",
        "  # loading the trained model from disk\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "    lsvm = pickle.load(file)\n",
        "  print(\"Loaded model from disk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wihQpGhaJA24"
      },
      "outputs": [],
      "source": [
        "y_pred = lsvm.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac = accuracy_score(y_test, y_pred)*100 # calculating accuracy of predicted data\n",
        "print(\"LSVM-Classifier Binary Set-Accuracy is \", ac)\n",
        "\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le1.classes_))\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dC0FQMEEb0A6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(y_pred[300:500], label=\"prediction\", linewidth=2.0,color='blue')\n",
        "plt.plot(y_test[300:500].values, label=\"real_values\", linewidth=2.0,color='lightcoral')\n",
        "plt.legend(loc=\"best\")\n",
        "plt.ylim((-1,2))\n",
        "plt.title(\"Linear SVM Binary Classification\")\n",
        "# plt.savefig('plots/lsvm_real_pred_bin.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzSuB-csds42"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "52r61mToLwbu"
      },
      "outputs": [],
      "source": [
        "knn=KNeighborsClassifier(n_neighbors=5) # creating model for 5 neighbors\n",
        "knn.fit(X_train,y_train) # training model on training dataset\n",
        "\n",
        "pkl_filename = \"knn_binary.pkl\"\n",
        "if (not path.isfile(pkl_filename)):\n",
        "  # saving the trained model to disk\n",
        "  with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(knn, file)\n",
        "  print(\"Saved model to disk\")\n",
        "  # loading the trained model from disk\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "    knn = pickle.load(file)\n",
        "  print(\"Loaded model from disk\")\n",
        "y_pred=knn.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100 # calculating accuracy of predicted data\n",
        "print(\"KNN-Classifier Binary Set-Accuracy is \", ac)\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le1.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jEHffvVod8HS"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "rows_with_nan = multi_data[multi_data.isnull().any(axis=1)]\n",
        "multi_data.dropna(inplace=True)\n",
        "X = multi_data.iloc[:,0:93].to_numpy() # dataset excluding target attribute (encoded, one-hot-encoded, original)\n",
        "Y = multi_data['intrusion'] # target attribute\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z6mlniBhe7xd"
      },
      "outputs": [],
      "source": [
        "lsvm=SVC(kernel='linear',gamma='auto')\n",
        "lsvm.fit(X_train,y_train) # training model on training dataset\n",
        "# saving trained model to disk\n",
        "pkl_filename = \"./lsvm_multi.pkl\"\n",
        "if (not path.isfile(pkl_filename)):\n",
        "  with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(lsvm, file)\n",
        "  print(\"Saved model to disk\")\n",
        "  # loading trained model from disk\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "    lsvm = pickle.load(file)\n",
        "  print(\"Loade model from disk\")\n",
        "\n",
        "y_pred=lsvm.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"LSVM-Classifier Multi-class Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le2.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U6GDrojDfZvF"
      },
      "outputs": [],
      "source": [
        "\n",
        "knn=KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train,y_train) # training model on training dataset\n",
        "\n",
        "pkl_filename = \"./knn_multi.pkl\"\n",
        "if (not path.isfile(pkl_filename)):\n",
        "  # saving trained model to disk\n",
        "  with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(knn, file)\n",
        "  print(\"Saved model to disk\")\n",
        "  # loading trained model from disk\n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "    knn = pickle.load(file)\n",
        "  print(\"Loaded model from disk\")\n",
        "y_pred=knn.predict(X_test)  # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"KNN-Classifier Multi-class Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le2.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5QUFkv5lO1P"
      },
      "source": [
        "testing starts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6vmDaXTIh5f5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the models\n",
        "with open(\"lsvm_binary.pkl\", \"rb\") as file:\n",
        "    binary_svm_model = pickle.load(file)\n",
        "\n",
        "with open(\"knn_binary.pkl\", \"rb\") as file:\n",
        "    binary_knn_model = pickle.load(file)\n",
        "\n",
        "with open(\"lsvm_multi.pkl\", \"rb\") as file:\n",
        "    multi_svm_model = pickle.load(file)\n",
        "\n",
        "with open(\"knn_multi.pkl\", \"rb\") as file:\n",
        "    multi_knn_model = pickle.load(file)\n",
        "\n",
        "# Load the LabelEncoders for decoding output\n",
        "le1_classes = np.load(\"le1_classes.npy\", allow_pickle=True)\n",
        "le2_classes = np.load(\"le2_classes.npy\", allow_pickle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z9A09gtd4afV"
      },
      "outputs": [],
      "source": [
        "# prompt: can you help me build a ml pipeline that takes raw data like from KDDTrain+.txt and then processes it like we did above and then gives it to modle which we loaded earlier and then show the output , do this and only for binary classification\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "\n",
        "# Assuming you have your trained models (binary_svm_model, binary_knn_model, etc.) loaded as in your previous code.\n",
        "# Also assuming you have le1_classes and le2_classes loaded.\n",
        "\n",
        "def process_raw_data(raw_data_string, model_type):\n",
        "  \"\"\"\n",
        "  Processes raw data string, prepares it for prediction, and returns the prediction.\n",
        "\n",
        "  Args:\n",
        "    raw_data_string: A string representing the raw data, like a line from KDDTrain+.txt\n",
        "    model_type: \"binary\" or \"multi\" depending on the model to be used.\n",
        "\n",
        "  Returns:\n",
        "    A prediction (class label) based on the input raw data.\n",
        "  \"\"\"\n",
        "  columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",]\n",
        "\n",
        "  # Convert raw data string to a list (split by commas)\n",
        "  raw_data_list = raw_data_string.split(',')\n",
        "\n",
        "  df_input = pd.DataFrame([raw_data_list])\n",
        "  df_input.columns = columns # Assign columns after DataFrame creation\n",
        "\n",
        "  # Change attack labels to classes (Dos, R2L, etc.)\n",
        "  def change_label(df):\n",
        "    df.label.replace(['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm'],'Dos',inplace=True)\n",
        "    df.label.replace(['ftp_write','guess_passwd','httptunnel','imap','multihop','named','phf','sendmail',\n",
        "         'snmpgetattack','snmpguess','spy','warezclient','warezmaster','xlock','xsnoop'],'R2L',inplace=True)\n",
        "    df.label.replace(['ipsweep','mscan','nmap','portsweep','saint','satan'],'Probe',inplace=True)\n",
        "    df.label.replace(['buffer_overflow','loadmodule','perl','ps','rootkit','sqlattack','xterm'],'U2R',inplace=True)\n",
        "  change_label(df_input)\n",
        "\n",
        "  # Normalize numeric features\n",
        "  numeric_col = df_input.select_dtypes(include='number').columns\n",
        "  std_scaler = StandardScaler()\n",
        "  for i in numeric_col:\n",
        "    arr = df_input[i]\n",
        "    arr = np.array(arr)\n",
        "    df_input[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
        "\n",
        "  # One-hot encode categorical features\n",
        "  cat_col = ['protocol_type','service','flag']\n",
        "  df_categorical = pd.get_dummies(df_input[cat_col], columns=cat_col)\n",
        "\n",
        "  # Prepare input for the chosen model type\n",
        "  if model_type == \"binary\":\n",
        "      bin_label = pd.DataFrame(df_input.label.map(lambda x:'normal' if x=='normal' else 'abnormal'))\n",
        "      df_input['intrusion'] = bin_label.apply(preprocessing.LabelEncoder().fit_transform)\n",
        "      df_input = df_input[['count','srv_serror_rate','serror_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "                         'logged_in','dst_host_same_srv_rate','dst_host_srv_count','same_srv_rate']]\n",
        "      df_input = df_input.join(df_categorical)\n",
        "      input_for_model = df_input.to_numpy()\n",
        "      prediction = binary__model.predict(input_for_model) # Assuming binary_svm_model is your loaded model.\n",
        "      predicted_class = le1_classes[prediction[0]]\n",
        "      return predicted_class\n",
        "  elif model_type == \"multi\":\n",
        "      # Similar steps as for binary, but using multi-class model and label encoder.\n",
        "      pass  # Add multi-class processing logic here.\n",
        "  else:\n",
        "      return \"Invalid model type.\"\n",
        "\n",
        "# Example usage\n",
        "raw_data = \"0,tcp,systat,S0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,239,20,1.00,1.00,0.00,0.00,0.08,0.07,0.00,255,20,0.08,0.08,0.00,0.00,1.00,1.00,0.00,0.00,anomaly\"\n",
        "prediction = process_raw_data(raw_data, \"binary\")\n",
        "print(\"Prediction:\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T85us9v36SbN"
      },
      "outputs": [],
      "source": [
        "def preprocess_input(raw_data, std_scaler, encoder):\n",
        "    # Split the input string and create a DataFrame\n",
        "    features = raw_data.split(',')\n",
        "\n",
        "    # Ensure features match the expected columns in the bin_data.csv dataset\n",
        "    feature_names = [\n",
        "        'count', 'protocol_type', 'service', 'flag', 'srv_serror_rate', 'serror_rate',\n",
        "        'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'logged_in',\n",
        "        'dst_host_same_srv_rate', 'dst_host_srv_count', 'same_srv_rate',\n",
        "        'dst_host_count', 'dst_host_srv_count', 'dst_host_diff_srv_rate',\n",
        "        'dst_host_same_src_port_rate', 'diff_srv_rate', 'same_srv_rate',\n",
        "        'intrusion', 'abnormal', 'normal'\n",
        "    ]\n",
        "\n",
        "    # Create DataFrame\n",
        "    input_df = pd.DataFrame([features], columns=feature_names)\n",
        "\n",
        "    # Convert appropriate columns to numeric\n",
        "    numeric_columns = [\n",
        "        'count', 'srv_serror_rate', 'serror_rate', 'dst_host_serror_rate',\n",
        "        'dst_host_srv_serror_rate', 'logged_in', 'dst_host_same_srv_rate',\n",
        "        'dst_host_srv_count', 'same_srv_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "        'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'diff_srv_rate'\n",
        "    ]\n",
        "\n",
        "    # Convert numeric columns to numeric type\n",
        "    input_df[numeric_columns] = input_df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "    # Normalize numeric data\n",
        "    input_df[numeric_columns] = std_scaler.transform(input_df[numeric_columns])\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    categorical_features = ['protocol_type', 'service', 'flag']\n",
        "    encoded_categorical = encoder.transform(input_df[categorical_features]).toarray()\n",
        "    encoded_columns = encoder.get_feature_names_out(categorical_features)\n",
        "\n",
        "    # Create a DataFrame for the encoded categorical features\n",
        "    encoded_df = pd.DataFrame(encoded_categorical, columns=encoded_columns)\n",
        "\n",
        "    # Combine the normalized numeric data with encoded categorical data\n",
        "    processed_data = pd.concat([input_df[numeric_columns], encoded_df], axis=1)\n",
        "\n",
        "    # Set placeholders for the label attributes; modify as needed\n",
        "    processed_data['intrusion'] = 'normal'  # Placeholder; modify as needed\n",
        "    processed_data['abnormal'] = 0  # Placeholder\n",
        "    processed_data['normal'] = 1  # Placeholder\n",
        "    processed_data['label'] = 'normal'  # Placeholder; modify as needed\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Example input\n",
        "raw_input = \"0,tcp,private,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,229,10,0.00,0.00,1.00,1.00,0.04,0.06,0.00,255,10,0.04,0.06,0.00,0.00,0.00,0.00,1.00,1.00,neptune,21\"\n",
        "# Use your pre-fitted std_scaler and encoder here\n",
        "processed_output = preprocess_input(raw_input, std_scaler, le1)\n",
        "print(processed_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}